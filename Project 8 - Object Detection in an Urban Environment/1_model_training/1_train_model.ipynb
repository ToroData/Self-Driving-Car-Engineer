{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1cd147",
   "metadata": {},
   "source": [
    "# Tensorflow Object Detection API and AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592c17",
   "metadata": {},
   "source": [
    "In this notebook, you will train and evaluate different models using the [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) and [AWS Sagemaker](https://aws.amazon.com/sagemaker/). \n",
    "\n",
    "If you ever feel stuck, you can refer to this [tutorial](https://aws.amazon.com/blogs/machine-learning/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker/).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are using the [Waymo Open Dataset](https://waymo.com/open/) for this project. The dataset has already been exported using the tfrecords format. The files have been created following the format described [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records). You can find data stored on [AWS S3](https://aws.amazon.com/s3/), AWS Object Storage. The images are saved with a resolution of 640x640."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc1d114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install tensorflow_io sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f55350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from framework import CustomFramework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccde6fd1",
   "metadata": {},
   "source": [
    "Save the IAM role in a variable called `role`. This would be useful when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab6b13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::263894401531:role/service-role/AmazonSageMaker-ExecutionRole-20230602T103300\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae64e29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The train and val paths below are public S3 buckets created by Udacity for this project\n",
    "inputs = {'train': 's3://cd2688-object-detection-tf2/train/', \n",
    "        'val': 's3://cd2688-object-detection-tf2/val/'} \n",
    "\n",
    "# Insert path of a folder in your personal S3 bucket to store tensorboard logs.\n",
    "tensorboard_s3_prefix = 's3://object-detection-project-2023/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16a825",
   "metadata": {},
   "source": [
    "## Container\n",
    "\n",
    "To train the model, you will first need to build a [docker](https://www.docker.com/) container with all the dependencies required by the TF Object Detection API. The code below does the following:\n",
    "* clone the Tensorflow models repository\n",
    "* get the exporter and training scripts from the the repository\n",
    "* build the docker image and push it \n",
    "* print the container name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad5ac8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'docker/models' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# clone the repo and get the scripts\n",
    "git clone https://github.com/tensorflow/models.git docker/models\n",
    "\n",
    "# get model_main and exporter_main files from TF2 Object Detection GitHub repository\n",
    "cp docker/models/research/object_detection/exporter_main_v2.py source_dir \n",
    "cp docker/models/research/object_detection/model_main_tf2.py source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dab3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build and push the docker image. This code can be commented after being ran once.\n",
    "# This will take around 10 mins.\n",
    "image_name = 'tf2-object-detection'\n",
    "!sh ./docker/build_and_push.sh $image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b3562",
   "metadata": {},
   "source": [
    "To verify that the image was correctly pushed to the [Elastic Container Registry](https://aws.amazon.com/ecr/), you can look at it in the AWS webapp. For example, below you can see that three different images have been pushed to ECR. You should only see one, called `tf2-object-detection`.\n",
    "![ECR Example](../data/example_ecr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0310b6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263894401531.dkr.ecr.us-east-1.amazonaws.com/tf2-object-detection:20230602095525\n"
     ]
    }
   ],
   "source": [
    "# display the container name\n",
    "with open (os.path.join('docker', 'ecr_image_fullname.txt'), 'r') as f:\n",
    "    container = f.readlines()[0][:-1]\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2a754",
   "metadata": {},
   "source": [
    "## Pre-trained model from model zoo\n",
    "\n",
    "As often, we are not training from scratch and we will be using a pretrained model from the TF Object Detection model zoo. You can find pretrained checkpoints [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Because your time is limited for this project, we recommend to only experiment with the following models:\n",
    "* SSD MobileNet V2 FPNLite 640x640\t\n",
    "* SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\t\n",
    "* Faster R-CNN ResNet50 V1 640x640\t\n",
    "* EfficientDet D1 640x640\t\n",
    "* Faster R-CNN ResNet152 V1 640x640\t\n",
    "\n",
    "In the code below, the EfficientDet D1 model is downloaded and extracted. This code should be ajusted if you were to experiment with other architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b1d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir /tmp/checkpoint\n",
    "mkdir source_dir/checkpoint\n",
    "wget -O /tmp/efficientdet.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz\n",
    "tar -zxvf /tmp/efficientdet.tar.gz --strip-components 2 --directory source_dir/checkpoint efficientdet_d1_coco17_tpu-32/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e04a98",
   "metadata": {},
   "source": [
    "## Edit pipeline.config file\n",
    "\n",
    "The [`pipeline.config`](source_dir/pipeline.config) in the `source_dir` folder should be updated when you experiment with different models. The different config files are available [here](https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2).\n",
    "\n",
    ">Note: The provided `pipeline.config` file works well with the `EfficientDet` model. You would need to modify it when working with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47483545",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "\n",
    "Now that we have a dataset, a docker image and some pretrained model weights, we can launch the training job. To do so, we create a [Sagemaker Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html), where we indicate the container name, name of the config file, number of training steps etc.\n",
    "\n",
    "The `run_training.sh` script does the following:\n",
    "* train the model for `num_train_steps` \n",
    "* evaluate over the val dataset\n",
    "* export the model\n",
    "\n",
    "Different metrics will be displayed during the evaluation phase, including the mean average precision. These metrics can be used to quantify your model performances and compare over the different iterations.\n",
    "\n",
    "You can also monitor the training progress by navigating to **Training -> Training Jobs** from the Amazon Sagemaker dashboard in the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7175cc",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf2-object-detection-2023-06-05-16-23-04-480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:23:12 Starting - Starting the training job...\n",
      "2023-06-05 16:23:28 Starting - Preparing the instances for training.........\n",
      "2023-06-05 16:25:03 Downloading - Downloading input data...\n",
      "2023-06-05 16:25:30 Training - Downloading the training image............\n",
      "2023-06-05 16:27:41 Training - Training image download completed. Training in progress....\u001b[34m2023-06-05 16:28:11,582 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,584 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,595 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,597 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,608 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,610 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,619 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/training\",\n",
      "        \"num_train_steps\": \"2000\",\n",
      "        \"pipeline_config_path\": \"pipeline.config\",\n",
      "        \"sample_1_of_n_eval_examples\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"tf2-object-detection-2023-06-05-16-23-04-480\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-263894401531/tf2-object-detection-2023-06-05-16-23-04-480/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_training.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_training.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/training\",\"num_train_steps\":\"2000\",\"pipeline_config_path\":\"pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.trn1.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-263894401531/tf2-object-detection-2023-06-05-16-23-04-480/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.trn1.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"/opt/training\",\"num_train_steps\":\"2000\",\"pipeline_config_path\":\"pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"tf2-object-detection-2023-06-05-16-23-04-480\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-263894401531/tf2-object-detection-2023-06-05-16-23-04-480/source/sourcedir.tar.gz\",\"module_name\":\"run_training.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_training.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/training\",\"--num_train_steps\",\"2000\",\"--pipeline_config_path\",\"pipeline.config\",\"--sample_1_of_n_eval_examples\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/training\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_STEPS=2000\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_CONFIG_PATH=pipeline.config\u001b[0m\n",
      "\u001b[34mSM_HP_SAMPLE_1_OF_N_EVAL_EXAMPLES=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run_training.sh --model_dir /opt/training --num_train_steps 2000 --pipeline_config_path pipeline.config --sample_1_of_n_eval_examples 1\"\u001b[0m\n",
      "\u001b[34m2023-06-05 16:28:11,619 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m===TRAINING THE MODEL==\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \u001b[0m\n",
      "\u001b[34mTensorFlow Addons (TFA) has ended development and introduction of new features.\u001b[0m\n",
      "\u001b[34mTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\u001b[0m\n",
      "\u001b[34mPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \u001b[0m\n",
      "\u001b[34mFor more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\u001b[0m\n",
      "\u001b[34mW0605 16:28:16.198755 140336315324224 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.215905 140336315324224 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting train_steps: 2000\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.218809 140336315324224 config_util.py:552] Maybe overwriting train_steps: 2000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.218908 140336315324224 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.227246 140336315324224 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.227329 140336315324224 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.227376 140336315324224 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.230628 140336315324224 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.266341 140336315324224 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.266421 140336315324224 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.409306 140336315324224 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.409392 140336315324224 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.674203 140336315324224 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.674286 140336315324224 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.934587 140336315324224 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 16:28:16.934670 140336315324224 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 16:28:17.283720 140336315324224 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 16:28:17.283810 140336315324224 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 16:28:17.633298 140336315324224 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 16:28:17.633381 140336315324224 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.197982 140336315324224 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.198105 140336315324224 efficientnet_model.py:143] round_filter input=320 output=320\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.387147 140336315324224 efficientnet_model.py:143] round_filter input=1280 output=1280\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.433351 140336315324224 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mW0605 16:28:18.477788 140336315324224 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.484185 140336315324224 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.485166 140336315324224 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mI0605 16:28:18.485259 140336315324224 dataset_builder.py:80] Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW0605 16:28:18.492247 140336315324224 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW0605 16:28:18.505265 140336315324224 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW0605 16:28:24.239403 140336315324224 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW0605 16:28:27.435488 140336315324224 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.\u001b[0m\n",
      "\u001b[34mW0605 16:28:29.324844 140336315324224 module_wrapper.py:149] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mI0605 16:28:34.682969 140331850909440 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI0605 16:28:43.496409 140331850909440 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mW0605 16:28:55.296344 140330699568896 deprecation.py:569] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mI0605 16:28:57.472703 140330699568896 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mW0605 16:29:03.397408 140330699568896 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mI0605 16:29:07.699216 140330699568896 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mW0605 16:29:13.544401 140330699568896 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mI0605 16:29:17.123848 140330699568896 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mW0605 16:29:22.964571 140330699568896 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mI0605 16:29:26.918426 140330699568896 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mW0605 16:29:32.348902 140330699568896 utils.py:82] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 100 per-step time 3.921s\u001b[0m\n",
      "\u001b[34mI0605 16:35:27.048564 140336315324224 model_lib_v2.py:705] Step 100 per-step time 3.921s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.45793518,\n",
      " 'Loss/localization_loss': 0.06368563,\n",
      " 'Loss/regularization_loss': 0.029540282,\n",
      " 'Loss/total_loss': 0.5511611,\n",
      " 'learning_rate': 0.00416}\u001b[0m\n",
      "\u001b[34mI0605 16:35:27.048886 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.45793518,\n",
      " 'Loss/localization_loss': 0.06368563,\n",
      " 'Loss/regularization_loss': 0.029540282,\n",
      " 'Loss/total_loss': 0.5511611,\n",
      " 'learning_rate': 0.00416}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 200 per-step time 3.381s\u001b[0m\n",
      "\u001b[34mI0605 16:41:05.129995 140336315324224 model_lib_v2.py:705] Step 200 per-step time 3.381s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.36672902,\n",
      " 'Loss/localization_loss': 0.020689037,\n",
      " 'Loss/regularization_loss': 0.029545657,\n",
      " 'Loss/total_loss': 0.41696373,\n",
      " 'learning_rate': 0.0073200003}\u001b[0m\n",
      "\u001b[34mI0605 16:41:05.130244 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.36672902,\n",
      " 'Loss/localization_loss': 0.020689037,\n",
      " 'Loss/regularization_loss': 0.029545657,\n",
      " 'Loss/total_loss': 0.41696373,\n",
      " 'learning_rate': 0.0073200003}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 300 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mI0605 16:46:43.326986 140336315324224 model_lib_v2.py:705] Step 300 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.28150973,\n",
      " 'Loss/localization_loss': 0.033771306,\n",
      " 'Loss/regularization_loss': 0.029552191,\n",
      " 'Loss/total_loss': 0.34483323,\n",
      " 'learning_rate': 0.010480001}\u001b[0m\n",
      "\u001b[34mI0605 16:46:43.327255 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.28150973,\n",
      " 'Loss/localization_loss': 0.033771306,\n",
      " 'Loss/regularization_loss': 0.029552191,\n",
      " 'Loss/total_loss': 0.34483323,\n",
      " 'learning_rate': 0.010480001}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 400 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 16:52:21.228332 140336315324224 model_lib_v2.py:705] Step 400 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.35716972,\n",
      " 'Loss/localization_loss': 0.023182176,\n",
      " 'Loss/regularization_loss': 0.029558366,\n",
      " 'Loss/total_loss': 0.40991026,\n",
      " 'learning_rate': 0.0136400005}\u001b[0m\n",
      "\u001b[34mI0605 16:52:21.228592 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.35716972,\n",
      " 'Loss/localization_loss': 0.023182176,\n",
      " 'Loss/regularization_loss': 0.029558366,\n",
      " 'Loss/total_loss': 0.40991026,\n",
      " 'learning_rate': 0.0136400005}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 500 per-step time 3.380s\u001b[0m\n",
      "\u001b[34mI0605 16:57:59.225694 140336315324224 model_lib_v2.py:705] Step 500 per-step time 3.380s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.24140465,\n",
      " 'Loss/localization_loss': 0.01731881,\n",
      " 'Loss/regularization_loss': 0.02956631,\n",
      " 'Loss/total_loss': 0.2882898,\n",
      " 'learning_rate': 0.016800001}\u001b[0m\n",
      "\u001b[34mI0605 16:57:59.225951 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.24140465,\n",
      " 'Loss/localization_loss': 0.01731881,\n",
      " 'Loss/regularization_loss': 0.02956631,\n",
      " 'Loss/total_loss': 0.2882898,\n",
      " 'learning_rate': 0.016800001}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 600 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 17:03:37.080097 140336315324224 model_lib_v2.py:705] Step 600 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.30420536,\n",
      " 'Loss/localization_loss': 0.018019987,\n",
      " 'Loss/regularization_loss': 0.029580794,\n",
      " 'Loss/total_loss': 0.35180613,\n",
      " 'learning_rate': 0.019960001}\u001b[0m\n",
      "\u001b[34mI0605 17:03:37.080378 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.30420536,\n",
      " 'Loss/localization_loss': 0.018019987,\n",
      " 'Loss/regularization_loss': 0.029580794,\n",
      " 'Loss/total_loss': 0.35180613,\n",
      " 'learning_rate': 0.019960001}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 700 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 17:09:14.932350 140336315324224 model_lib_v2.py:705] Step 700 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.33903533,\n",
      " 'Loss/localization_loss': 0.018474929,\n",
      " 'Loss/regularization_loss': 0.029603958,\n",
      " 'Loss/total_loss': 0.38711423,\n",
      " 'learning_rate': 0.023120001}\u001b[0m\n",
      "\u001b[34mI0605 17:09:14.932612 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.33903533,\n",
      " 'Loss/localization_loss': 0.018474929,\n",
      " 'Loss/regularization_loss': 0.029603958,\n",
      " 'Loss/total_loss': 0.38711423,\n",
      " 'learning_rate': 0.023120001}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 800 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 17:14:52.794992 140336315324224 model_lib_v2.py:705] Step 800 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.3618258,\n",
      " 'Loss/localization_loss': 0.025779154,\n",
      " 'Loss/regularization_loss': 0.029635146,\n",
      " 'Loss/total_loss': 0.41724008,\n",
      " 'learning_rate': 0.02628}\u001b[0m\n",
      "\u001b[34mI0605 17:14:52.795277 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.3618258,\n",
      " 'Loss/localization_loss': 0.025779154,\n",
      " 'Loss/regularization_loss': 0.029635146,\n",
      " 'Loss/total_loss': 0.41724008,\n",
      " 'learning_rate': 0.02628}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 900 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mI0605 17:20:31.032622 140336315324224 model_lib_v2.py:705] Step 900 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.28774184,\n",
      " 'Loss/localization_loss': 0.014149195,\n",
      " 'Loss/regularization_loss': 0.029663146,\n",
      " 'Loss/total_loss': 0.33155417,\n",
      " 'learning_rate': 0.02944}\u001b[0m\n",
      "\u001b[34mI0605 17:20:31.032884 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.28774184,\n",
      " 'Loss/localization_loss': 0.014149195,\n",
      " 'Loss/regularization_loss': 0.029663146,\n",
      " 'Loss/total_loss': 0.33155417,\n",
      " 'learning_rate': 0.02944}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1000 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mI0605 17:26:09.199777 140336315324224 model_lib_v2.py:705] Step 1000 per-step time 3.382s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.2960887,\n",
      " 'Loss/localization_loss': 0.017049924,\n",
      " 'Loss/regularization_loss': 0.029713282,\n",
      " 'Loss/total_loss': 0.34285188,\n",
      " 'learning_rate': 0.0326}\u001b[0m\n",
      "\u001b[34mI0605 17:26:09.200046 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.2960887,\n",
      " 'Loss/localization_loss': 0.017049924,\n",
      " 'Loss/regularization_loss': 0.029713282,\n",
      " 'Loss/total_loss': 0.34285188,\n",
      " 'learning_rate': 0.0326}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1100 per-step time 3.410s\u001b[0m\n",
      "\u001b[34mI0605 17:31:50.218647 140336315324224 model_lib_v2.py:705] Step 1100 per-step time 3.410s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.305284,\n",
      " 'Loss/localization_loss': 0.021742895,\n",
      " 'Loss/regularization_loss': 0.029763255,\n",
      " 'Loss/total_loss': 0.35679016,\n",
      " 'learning_rate': 0.03576}\u001b[0m\n",
      "\u001b[34mI0605 17:31:50.218926 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.305284,\n",
      " 'Loss/localization_loss': 0.021742895,\n",
      " 'Loss/regularization_loss': 0.029763255,\n",
      " 'Loss/total_loss': 0.35679016,\n",
      " 'learning_rate': 0.03576}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1200 per-step time 3.388s\u001b[0m\n",
      "\u001b[34mI0605 17:37:28.998044 140336315324224 model_lib_v2.py:705] Step 1200 per-step time 3.388s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.27945623,\n",
      " 'Loss/localization_loss': 0.015509144,\n",
      " 'Loss/regularization_loss': 0.029828366,\n",
      " 'Loss/total_loss': 0.32479376,\n",
      " 'learning_rate': 0.03892}\u001b[0m\n",
      "\u001b[34mI0605 17:37:28.998289 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.27945623,\n",
      " 'Loss/localization_loss': 0.015509144,\n",
      " 'Loss/regularization_loss': 0.029828366,\n",
      " 'Loss/total_loss': 0.32479376,\n",
      " 'learning_rate': 0.03892}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1300 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 17:43:06.856248 140336315324224 model_lib_v2.py:705] Step 1300 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.26329976,\n",
      " 'Loss/localization_loss': 0.016449178,\n",
      " 'Loss/regularization_loss': 0.029893603,\n",
      " 'Loss/total_loss': 0.30964255,\n",
      " 'learning_rate': 0.04208}\u001b[0m\n",
      "\u001b[34mI0605 17:43:06.856498 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.26329976,\n",
      " 'Loss/localization_loss': 0.016449178,\n",
      " 'Loss/regularization_loss': 0.029893603,\n",
      " 'Loss/total_loss': 0.30964255,\n",
      " 'learning_rate': 0.04208}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1400 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 17:48:44.743083 140336315324224 model_lib_v2.py:705] Step 1400 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.24272576,\n",
      " 'Loss/localization_loss': 0.019056318,\n",
      " 'Loss/regularization_loss': 0.0299799,\n",
      " 'Loss/total_loss': 0.291762,\n",
      " 'learning_rate': 0.04524}\u001b[0m\n",
      "\u001b[34mI0605 17:48:44.743356 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.24272576,\n",
      " 'Loss/localization_loss': 0.019056318,\n",
      " 'Loss/regularization_loss': 0.0299799,\n",
      " 'Loss/total_loss': 0.291762,\n",
      " 'learning_rate': 0.04524}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1500 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mI0605 17:54:22.585915 140336315324224 model_lib_v2.py:705] Step 1500 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.20999502,\n",
      " 'Loss/localization_loss': 0.009431966,\n",
      " 'Loss/regularization_loss': 0.030065142,\n",
      " 'Loss/total_loss': 0.24949212,\n",
      " 'learning_rate': 0.0484}\u001b[0m\n",
      "\u001b[34mI0605 17:54:22.586165 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.20999502,\n",
      " 'Loss/localization_loss': 0.009431966,\n",
      " 'Loss/regularization_loss': 0.030065142,\n",
      " 'Loss/total_loss': 0.24949212,\n",
      " 'learning_rate': 0.0484}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1600 per-step time 3.377s\u001b[0m\n",
      "\u001b[34mI0605 18:00:00.237303 140336315324224 model_lib_v2.py:705] Step 1600 per-step time 3.377s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.22298996,\n",
      " 'Loss/localization_loss': 0.01420866,\n",
      " 'Loss/regularization_loss': 0.030152304,\n",
      " 'Loss/total_loss': 0.26735094,\n",
      " 'learning_rate': 0.05156}\u001b[0m\n",
      "\u001b[34mI0605 18:00:00.237568 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.22298996,\n",
      " 'Loss/localization_loss': 0.01420866,\n",
      " 'Loss/regularization_loss': 0.030152304,\n",
      " 'Loss/total_loss': 0.26735094,\n",
      " 'learning_rate': 0.05156}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1700 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mI0605 18:05:38.058159 140336315324224 model_lib_v2.py:705] Step 1700 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.2692067,\n",
      " 'Loss/localization_loss': 0.016461732,\n",
      " 'Loss/regularization_loss': 0.03024537,\n",
      " 'Loss/total_loss': 0.3159138,\n",
      " 'learning_rate': 0.05472}\u001b[0m\n",
      "\u001b[34mI0605 18:05:38.058427 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.2692067,\n",
      " 'Loss/localization_loss': 0.016461732,\n",
      " 'Loss/regularization_loss': 0.03024537,\n",
      " 'Loss/total_loss': 0.3159138,\n",
      " 'learning_rate': 0.05472}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1800 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 18:11:15.989342 140336315324224 model_lib_v2.py:705] Step 1800 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.27977288,\n",
      " 'Loss/localization_loss': 0.018595837,\n",
      " 'Loss/regularization_loss': 0.03035485,\n",
      " 'Loss/total_loss': 0.32872358,\n",
      " 'learning_rate': 0.05788}\u001b[0m\n",
      "\u001b[34mI0605 18:11:15.989585 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.27977288,\n",
      " 'Loss/localization_loss': 0.018595837,\n",
      " 'Loss/regularization_loss': 0.03035485,\n",
      " 'Loss/total_loss': 0.32872358,\n",
      " 'learning_rate': 0.05788}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1900 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mI0605 18:16:53.830339 140336315324224 model_lib_v2.py:705] Step 1900 per-step time 3.378s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.22024797,\n",
      " 'Loss/localization_loss': 0.008283604,\n",
      " 'Loss/regularization_loss': 0.030487653,\n",
      " 'Loss/total_loss': 0.25901923,\n",
      " 'learning_rate': 0.06104}\u001b[0m\n",
      "\u001b[34mI0605 18:16:53.830598 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.22024797,\n",
      " 'Loss/localization_loss': 0.008283604,\n",
      " 'Loss/regularization_loss': 0.030487653,\n",
      " 'Loss/total_loss': 0.25901923,\n",
      " 'learning_rate': 0.06104}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 2000 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mI0605 18:22:31.685037 140336315324224 model_lib_v2.py:705] Step 2000 per-step time 3.379s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.26322588,\n",
      " 'Loss/localization_loss': 0.012559994,\n",
      " 'Loss/regularization_loss': 0.030591251,\n",
      " 'Loss/total_loss': 0.3063771,\n",
      " 'learning_rate': 0.06420001}\u001b[0m\n",
      "\u001b[34mI0605 18:22:31.685315 140336315324224 model_lib_v2.py:708] {'Loss/classification_loss': 0.26322588,\n",
      " 'Loss/localization_loss': 0.012559994,\n",
      " 'Loss/regularization_loss': 0.030591251,\n",
      " 'Loss/total_loss': 0.3063771,\n",
      " 'learning_rate': 0.06420001}\u001b[0m\n",
      "\u001b[34m==EVALUATING THE MODEL==\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \u001b[0m\n",
      "\u001b[34mTensorFlow Addons (TFA) has ended development and introduction of new features.\u001b[0m\n",
      "\u001b[34mTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\u001b[0m\n",
      "\u001b[34mPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \u001b[0m\n",
      "\u001b[34mFor more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mW0605 18:22:38.753525 139922499462976 model_lib_v2.py:1089] Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.753668 139922499462976 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.753723 139922499462976 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.753785 139922499462976 config_util.py:552] Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mW0605 18:22:38.753862 139922499462976 model_lib_v2.py:1106] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.778220 139922499462976 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.778310 139922499462976 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.778361 139922499462976 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.781567 139922499462976 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.814472 139922499462976 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.814552 139922499462976 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.931811 139922499462976 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 18:22:38.931891 139922499462976 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.142769 139922499462976 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.142850 139922499462976 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.352075 139922499462976 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.352160 139922499462976 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.630740 139922499462976 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.630821 139922499462976 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.907275 139922499462976 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 18:22:39.907356 139922499462976 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.367774 139922499462976 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.367900 139922499462976 efficientnet_model.py:143] round_filter input=320 output=320\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.521709 139922499462976 efficientnet_model.py:143] round_filter input=1280 output=1280\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.561594 139922499462976 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.613103 139922499462976 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.613981 139922499462976 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mI0605 18:22:40.614074 139922499462976 dataset_builder.py:80] Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mW0605 18:22:40.614124 139922499462976 dataset_builder.py:86] num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:`shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mW0605 18:22:40.616362 139922499462976 dataset_builder.py:93] `shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW0605 18:22:40.618447 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW0605 18:22:40.631266 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW0605 18:22:44.057421 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW0605 18:22:45.076607 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.\u001b[0m\n",
      "\u001b[34mW0605 18:22:47.334429 139922499462976 module_wrapper.py:149] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/optimizer_builder.py:124: The name tf.keras.optimizers.SGD is deprecated. Please use tf.keras.optimizers.legacy.SGD instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI0605 18:22:47.334705 139922499462976 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Found new checkpoint at /opt/training/ckpt-3\u001b[0m\n",
      "\u001b[34mI0605 18:22:47.335130 139922499462976 checkpoint_utils.py:177] Found new checkpoint at /opt/training/ckpt-3\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mI0605 18:22:56.074560 139922499462976 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI0605 18:23:07.857630 139922499462976 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW0605 18:23:12.376345 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 0\u001b[0m\n",
      "\u001b[34mI0605 18:23:12.391273 139922499462976 model_lib_v2.py:966] Finished eval step 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mW0605 18:23:12.508586 139922499462976 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 100\u001b[0m\n",
      "\u001b[34mI0605 18:23:29.524350 139922499462976 model_lib_v2.py:966] Finished eval step 100\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 200\u001b[0m\n",
      "\u001b[34mI0605 18:23:44.288470 139922499462976 model_lib_v2.py:966] Finished eval step 200\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mI0605 18:23:52.715992 139922499462976 coco_evaluation.py:293] Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mI0605 18:23:52.721440 139922499462976 coco_tools.py:116] Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:DONE (t=0.01s)\u001b[0m\n",
      "\u001b[34mI0605 18:23:52.734229 139922499462976 coco_tools.py:138] DONE (t=0.01s)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Eval metrics at step 2000\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.104842 139922499462976 model_lib_v2.py:1015] Eval metrics at step 2000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP: 0.080789\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.116414 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP: 0.080789\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.50IOU: 0.199362\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.117155 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.50IOU: 0.199362\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.75IOU: 0.056819\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.117731 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.75IOU: 0.056819\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (small): 0.030616\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.118256 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (small): 0.030616\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (medium): 0.285764\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.118759 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (medium): 0.285764\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (large): 0.310486\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.119252 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (large): 0.310486\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@1: 0.019847\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.119741 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@1: 0.019847\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@10: 0.088191\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.120212 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@10: 0.088191\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100: 0.129586\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.120669 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100: 0.129586\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (small): 0.074782\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.121172 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (small): 0.074782\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (medium): 0.385008\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.121647 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (medium): 0.385008\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (large): 0.523764\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.122139 139922499462976 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (large): 0.523764\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/localization_loss: 0.021285\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.122551 139922499462976 model_lib_v2.py:1018] #011+ Loss/localization_loss: 0.021285\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/classification_loss: 0.309844\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.122968 139922499462976 model_lib_v2.py:1018] #011+ Loss/classification_loss: 0.309844\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/regularization_loss: 0.030593\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.123371 139922499462976 model_lib_v2.py:1018] #011+ Loss/regularization_loss: 0.030593\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/total_loss: 0.361721\u001b[0m\n",
      "\u001b[34mI0605 18:24:00.123769 139922499462976 model_lib_v2.py:1018] #011+ Loss/total_loss: 0.361721\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI0605 18:27:47.427738 139922499462976 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mI0605 18:27:56.439831 139922499462976 checkpoint_utils.py:231] Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mRunning per image evaluation...\u001b[0m\n",
      "\u001b[34mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[34mDONE (t=7.13s).\u001b[0m\n",
      "\u001b[34mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[34mDONE (t=0.21s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.199\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.057\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.286\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.310\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.088\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.524\u001b[0m\n",
      "\u001b[34m==EXPORTING THE MODEL==\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \u001b[0m\n",
      "\u001b[34mTensorFlow Addons (TFA) has ended development and introduction of new features.\u001b[0m\n",
      "\u001b[34mTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\u001b[0m\n",
      "\u001b[34mPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \u001b[0m\n",
      "\u001b[34mFor more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.586976 140249788606272 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.587104 140249788606272 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.587164 140249788606272 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.590459 140249788606272 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.624768 140249788606272 efficientnet_model.py:143] round_filter input=32 output=32\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.624856 140249788606272 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.745583 140249788606272 efficientnet_model.py:143] round_filter input=16 output=16\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.745672 140249788606272 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.962357 140249788606272 efficientnet_model.py:143] round_filter input=24 output=24\u001b[0m\n",
      "\u001b[34mI0605 18:27:59.962451 140249788606272 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.174732 140249788606272 efficientnet_model.py:143] round_filter input=40 output=40\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.174814 140249788606272 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.450676 140249788606272 efficientnet_model.py:143] round_filter input=80 output=80\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.450760 140249788606272 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.732096 140249788606272 efficientnet_model.py:143] round_filter input=112 output=112\u001b[0m\n",
      "\u001b[34mI0605 18:28:00.732188 140249788606272 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 18:28:01.089131 140249788606272 efficientnet_model.py:143] round_filter input=192 output=192\u001b[0m\n",
      "\u001b[34mI0605 18:28:01.089224 140249788606272 efficientnet_model.py:143] round_filter input=320 output=320\u001b[0m\n",
      "\u001b[34mI0605 18:28:01.239964 140249788606272 efficientnet_model.py:143] round_filter input=1280 output=1280\u001b[0m\n",
      "\u001b[34mI0605 18:28:01.279799 140249788606272 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mW0605 18:28:04.860315 140249788606272 deprecation.py:641] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mI0605 18:28:08.759920 140249788606272 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI0605 18:28:17.779058 140249788606272 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI0605 18:28:22.931506 140249788606272 api.py:459] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f8dec491f40>, because it is not built.\u001b[0m\n",
      "\u001b[34mW0605 18:28:24.994908 140249788606272 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f8dec491f40>, because it is not built.\u001b[0m\n",
      "\u001b[34mW0605 18:29:08.063047 140249788606272 save.py:274] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 535). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mI0605 18:29:36.590055 140249788606272 builder_impl.py:797] Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34mI0605 18:29:39.935261 140249788606272 config_util.py:253] Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34m2023-06-05 18:29:42,568 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-06-05 18:30:09 Uploading - Uploading generated training model\n",
      "2023-06-05 18:30:09 Completed - Training job completed\n",
      "Training seconds: 7506\n",
      "Billable seconds: 7506\n"
     ]
    }
   ],
   "source": [
    "tensorboard_output_config = sagemaker.debugger.TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_s3_prefix,\n",
    "    container_local_output_path='/opt/training/'\n",
    ")\n",
    "\n",
    "estimator = CustomFramework(\n",
    "    role=role,\n",
    "    image_uri=container,\n",
    "    entry_point='run_training.sh',\n",
    "    source_dir='source_dir/',\n",
    "    hyperparameters={\n",
    "        \"model_dir\":\"/opt/training\",        \n",
    "        \"pipeline_config_path\": \"pipeline.config\",\n",
    "        \"num_train_steps\": \"2000\",    \n",
    "        \"sample_1_of_n_eval_examples\": \"1\"\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type='ml.trn1.2xlarge',\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    disable_profiler=True,\n",
    "    base_job_name='tf2-object-detection'\n",
    ")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84545881",
   "metadata": {},
   "source": [
    "You should be able to see your model training in the AWS webapp as shown below:\n",
    "![ECR Example](../data/example_trainings.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9844f25",
   "metadata": {},
   "source": [
    "## Improve on the intial model\n",
    "\n",
    "Most likely, this initial experiment did not yield optimal results. However, you can make multiple changes to the `pipeline.config` file to improve this model. One obvious change consists in improving the data augmentation strategy. The [`preprocessor.proto`](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto) file contains the different data augmentation method available in the Tf Object Detection API. Justify your choices of augmentations in the writeup.\n",
    "\n",
    "Keep in mind that the following are also available:\n",
    "* experiment with the optimizer: type of optimizer, learning rate, scheduler etc\n",
    "* experiment with the architecture. The Tf Object Detection API model zoo offers many architectures. Keep in mind that the pipeline.config file is unique for each architecture and you will have to edit it.\n",
    "* visualize results on the test frames using the `2_deploy_model` notebook available in this repository.\n",
    "\n",
    "In the cell below, write down all the different approaches you have experimented with, why you have chosen them and what you would have done if you had more time and resources. Justify your choices using the tensorboard visualizations (take screenshots and insert them in your writeup), the metrics on the evaluation set and the generated animation you have created with [this tool](../2_run_inference/2_deploy_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17284a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your writeup goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
